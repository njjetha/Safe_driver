# -*- coding: utf-8 -*-
"""xgboost-final-project-tested-0-21937.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Unm5G3lqBxt2hvnmIpsc7ovIFeI7TluW
"""

# import the necessary packages
import sys
import subprocess
subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'argparse'])
import argparse
# construct the argument parser and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-t","--train_set", required=True,help="path to train data")
ap.add_argument("-d","--test_set", required=True,help="path to test data")
args = vars(ap.parse_args())
print("---------------Parsing cmd line arguments---------------")
train_path = args["train_set"]
test_path = args["test_set"]
print(train_path)
print(test_path)



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer

# Data Loading
print("---------------Loading data---------------")
# train_i = pd.read_csv('train1.csv')
train = pd.read_csv(args["train_set"])
#test = pd.read_csv('test1.csv')
test = pd.read_csv(args["test_set"])
test1 = pd.read_csv(args["test_set"])

# train=pd.read_csv('../input/project/train.csv/train.csv')
# test=pd.read_csv('../input/project/test.csv/test.csv')

# test1=pd.read_csv('../input/project/test.csv/test.csv')

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, roc_auc_score ,roc_curve,accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegressionCV

train.info()

train

# 'ps_car_03_cat', 'ps_car_05_cat' have high percent of null value so we drop from both train and test dataset

col_drop=['ps_car_03_cat', 'ps_car_05_cat']
train.drop(col_drop, inplace=True, axis=1)
test.drop(col_drop, inplace=True, axis=1)

train.info()

# replacing -1 with NAN
train = train.replace(-1, np.NaN)

train.isna().sum().sort_values(ascending=False).head(15)

# features having null values 
null_values_train=['ps_reg_03','ps_car_14','ps_car_07_cat','ps_ind_05_cat','ps_car_09_cat','ps_ind_02_cat','ps_car_01_cat','ps_ind_04_cat','ps_car_02_cat','ps_car_11']

# filling null values with mean
for col in null_values_train:
    train[col] = train[col].fillna((train[col].mean()))

# replacing -1 with NAN
test=test.replace(-1, np.NaN)

test.isna().sum().sort_values(ascending=False).head(15)

null_values_test=['ps_reg_03','ps_car_14','ps_car_07_cat','ps_ind_05_cat','ps_car_09_cat','ps_ind_02_cat','ps_car_01_cat','ps_ind_04_cat','ps_car_02_cat','ps_car_11','ps_car_12']

# replacing null values with mean
for col in null_values_test:
    test[col] = test[col].fillna((test[col].mean()))

train.info()

# Features having categorical value 
categorical_col=['ps_ind_02_cat','ps_ind_04_cat','ps_ind_05_cat','ps_car_01_cat','ps_car_02_cat','ps_car_04_cat','ps_car_06_cat',
 'ps_car_07_cat','ps_car_08_cat','ps_car_09_cat','ps_car_10_cat','ps_car_11_cat']

# Performing label Encoding on categorical data on both train and test dataset .
le=LabelEncoder()
for col in categorical_col:
  train[col]=le.fit_transform(train[col])

for col in categorical_col:
  test[col]=le.fit_transform(test[col])

train.info()

test.info()

train_new=train
test_new=test

#Dropping id featuers from both train and test dataset 
col_drop=['id']
train_new.drop(col_drop, inplace=True, axis=1)
test_new.drop(col_drop, inplace=True, axis=1)

train_new.info()

test_new.info()

X = train_new.drop(['target'],axis=1)
y= train_new['target']

import gc

gc.collect()

def ginic(actual,pred):
    actual=np.asarray(actual)
    n=len(actual)
    a_s=actual[np.argsort(pred)]
    a_c=a_s.cumsum()
    ginisum=a_c.sum() / a_s.sum() - (n+1) / 2.0
    return ginisum / n

def gini_normalized(a,p):
    if p.ndim == 2:
        p = p[:,1]
    return ginic(a,p) / ginic(a,a)



from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import classification_report
from sklearn.metrics import roc_auc_score
import time

# create a 80/20 split of the data 
# xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.2)

import xgboost as xgb

clf_xgb = xgb.XGBClassifier(learning_rate=0.01, 
                            n_estimators=3000, 
                            max_depth=4,                    
                            subsample=0.9,
                            colsample_bytree=0.6,
                            objective= 'binary:logistic',
                            reg_alpha = 0,
                            reg_lambda = 1,
                            seed=42)
                            

# clf_xgb.fit(xtrain, ytrain, eval_set=[(xtrain, ytrain), (xvalid, yvalid)], 
#             early_stopping_rounds=100, eval_metric='auc', verbose=100)

# predictions = clf_xgb.predict(xvalid)
# print(classification_report(yvalid, predictions))
# print()
# print("accuracy_score", accuracy_score(yvalid, predictions))
# predictions_probas = clf_xgb.predict_proba(xvalid)
# gc.collect()

#gini score 
# score=gini_normalized(yvalid,predictions_probas)
# print(score)

# print('Confusion matrix\n',confusion_matrix(yvalid,predictions))

X_out=test_new

# result = clf_xgb.predict_proba(X_out)
# result

# id=test1['id']
# submit=pd.DataFrame({'id':id,'target':result[:,1]})
# submit=submit[['id','target']]

# submit.to_csv("Xgboost_3000_0.01_9params_testtrain.csv", index = False)

# submit.head(10)

X_train1 = train_new.drop(['target'],axis=1)
Y_train1= train_new['target']

clf_xgb.fit(X_train1, Y_train1)


print()
gc.collect()

result = clf_xgb.predict_proba(X_out)
result

id=test1['id']
submit=pd.DataFrame({'id':id,'target':result[:,1]})
submit=submit[['id','target']]

submit.to_csv("Xgboost_3000_0.01_9params_trainfull.csv", index = False)

#submit.head(10)

